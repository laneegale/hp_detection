{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbdc5305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trident.patch_encoder_models import encoder_factory\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "093edd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_encoders = [\n",
    "    \"uni_v1\",\n",
    "    \"uni_v2\",\n",
    "    \"conch_v1\",\n",
    "    \"conch_v15\",\n",
    "    \"virchow\",\n",
    "    \"virchow2\",\n",
    "    \"phikon\",\n",
    "    \"phikon_v2\",\n",
    "    \"gigapath\",\n",
    "    \"hoptimus0\",\n",
    "    \"hoptimus1\",\n",
    "    \"musk\",\n",
    "    \"midnight12k\",\n",
    "    \"kaiko-vits8\",\n",
    "    \"kaiko-vits16\",\n",
    "    \"kaiko-vitb8\",\n",
    "    \"kaiko-vitb16\",\n",
    "    \"kaiko-vitl14\",\n",
    "    \"lunit-vits8\",\n",
    "    \"hibou_l\",\n",
    "    \"ctranspath\",\n",
    "    \"resnet50\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e79425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Load the weights and the model\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "\n",
    "# This model is much \"leaner\" than the transformers version\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7230dbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18aebc6fb4994c399d43be655a3fd622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = encoder_factory(model_name='resnet50')\n",
    "\n",
    "trnsfrms_val = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45124832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0390, -1.0048, -0.9192,  ..., -1.3987, -0.7479,  0.0056],\n",
       "         [-0.9020, -0.9020, -0.9192,  ..., -1.2788, -1.2274, -0.5082],\n",
       "         [-0.9363, -0.8507, -0.7822,  ..., -1.0904, -1.0904, -0.7822],\n",
       "         ...,\n",
       "         [ 0.4508,  0.3138,  0.6734,  ...,  1.0159,  0.9303,  0.7591],\n",
       "         [ 0.9132,  0.4508,  0.9303,  ...,  0.8618,  0.8447,  0.7762],\n",
       "         [ 0.9988,  0.5878,  1.1872,  ...,  1.0502,  1.3242,  1.2214]],\n",
       "\n",
       "        [[-1.5805, -1.5455, -1.5105,  ..., -1.6506, -1.2654, -0.9853],\n",
       "         [-1.5280, -1.4930, -1.4755,  ..., -1.7031, -1.5630, -1.1954],\n",
       "         [-1.4930, -1.4580, -1.4055,  ..., -1.5980, -1.5630, -1.3880],\n",
       "         ...,\n",
       "         [-0.6352, -0.7577, -0.3725,  ..., -0.4951, -0.5301, -0.6176],\n",
       "         [-0.3901, -0.6001, -0.0924,  ..., -0.6527, -0.6702, -0.6877],\n",
       "         [-0.3375, -0.3725,  0.1176,  ..., -0.6176, -0.4951, -0.5301]],\n",
       "\n",
       "        [[-0.3230, -0.3055, -0.2358,  ..., -0.2707,  0.2871,  0.4439],\n",
       "         [-0.2184, -0.2010, -0.2010,  ..., -0.4973, -0.1487,  0.2696],\n",
       "         [-0.1487, -0.0964, -0.1138,  ..., -0.4101, -0.3230,  0.0953],\n",
       "         ...,\n",
       "         [ 0.7054,  0.6531,  0.9494,  ...,  0.8274,  0.7925,  0.7054],\n",
       "         [ 0.9494,  0.7925,  1.1062,  ...,  0.6705,  0.6879,  0.6531],\n",
       "         [ 1.0539,  0.8971,  1.1759,  ...,  0.6182,  0.7576,  0.7228]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "from PIL import Image, ImageFile, PngImagePlugin\n",
    "Image.MAX_IMAGE_PIXELS = None \n",
    "PngImagePlugin.MAX_TEXT_CHUNK = 100 * 1024 * 1024  # 100MB\n",
    "PngImagePlugin.MAX_TEXT_MEMORY = 100 * 1024 * 1024 # 100MB\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "path = \"/Z/cuhk_data/HPACG/train/positive/23S002833-II.svs_72704_29184_0_512_512_1.png\"\n",
    "image = Image.open(path).convert(\"RGB\")\n",
    "trnsfrms_val(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67867d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.stack([trnsfrms_val(image), trnsfrms_val(image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be744879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56555718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CTransPathInferenceEncoder(\n",
       "  (model): SwinTransformer(\n",
       "    (patch_embed): ConvStem(\n",
       "      (proj): Sequential(\n",
       "        (0): Conv2d(3, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(12, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (0): BasicLayer(\n",
       "        dim=96, input_resolution=(56, 56), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(56, 56), dim=96\n",
       "          (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicLayer(\n",
       "        dim=192, input_resolution=(28, 28), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(28, 28), dim=192\n",
       "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer(\n",
       "        dim=384, input_resolution=(14, 14), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0-5): 6 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(14, 14), dim=384\n",
       "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): BasicLayer(\n",
       "        dim=768, input_resolution=(7, 7), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "    (head): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9d26036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1538,  0.1100, -0.7045,  ...,  0.4339, -0.0437,  0.1632],\n",
       "        [ 0.1359, -0.0684, -0.5446,  ...,  0.5155, -0.2026,  0.3028]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f85b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6fafe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waiting permission\n",
    "\n",
    "model = timm.create_model(\n",
    "    'hf-hub:Wangyh/mSTAR',\n",
    "    pretrained=True,\n",
    "    init_values=1e-5, dynamic_img_size=True\n",
    "    )\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b04716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"histai/hibou-L\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"histai/hibou-L\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40805407",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoder_factory(model_name='virchow2')\n",
    "\n",
    "trnsfrms_val = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = encoder_factory(model_name='ctranspath')\n",
    "\n",
    "trnsfrms_val = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = encoder_factory(model_name='hoptimus0')\n",
    "\n",
    "trnsfrms_val = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.707223, 0.578729, 0.703617), \n",
    "        std=(0.211883, 0.230117, 0.177517)\n",
    "    ),\n",
    "])\n",
    "\n",
    "model = encoder_factory(model_name='hoptimus1')\n",
    "\n",
    "trnsfrms_val = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.707223, 0.578729, 0.703617), \n",
    "        std=(0.211883, 0.230117, 0.177517)\n",
    "    ),\n",
    "])\n",
    "\n",
    "model = encoder_factory(model_name='uni_v2')\n",
    "\n",
    "transform = transforms.Compose(\n",
    " [\n",
    "  transforms.Resize(224),\n",
    "  transforms.CenterCrop(224),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    " ]\n",
    ")\n",
    "\n",
    "model = encoder_factory(model_name='uni_v1')\n",
    "\n",
    "transform = transforms.Compose(\n",
    " [\n",
    "  transforms.Resize(224),\n",
    "  transforms.CenterCrop(224),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    " ]\n",
    ")\n",
    "\n",
    "from timm.data.constants import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "\n",
    "model = encoder_factory(model_name='musk')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(384, interpolation=3, antialias=True),\n",
    "    transforms.CenterCrop((384, 384)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD)\n",
    "])\n",
    "\n",
    "model = encoder_factory(model_name='conch_v15')\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            448, interpolation=transforms.InterpolationMode.BICUBIC\n",
    "        ),\n",
    "        transforms.CenterCrop(448),\n",
    "        # transforms.Lambda(\n",
    "        #     lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img\n",
    "        # ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6289be1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43c4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da7c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6860695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5677a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa3e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3d12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f80814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trident",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
