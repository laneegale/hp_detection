{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98dbb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "dataDir = Path(\"/media/toom/New Volume/cuhk_data/HPACG_dataHPACG_split\")\n",
    "train_positive_dir = dataDir / \"train/positive\"\n",
    "train_negative_dir = dataDir / \"train/negative\"\n",
    "\n",
    "test_positive_dir = dataDir / \"test/positive\"\n",
    "test_negative_dir = dataDir / \"test/negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ac022",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load image processor for 'mahmoodLab/conchv1_5'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mahmoodLab/conchv1_5' is the correct path to a directory containing a preprocessor_config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl13/lib/python3.13/site-packages/transformers/image_processing_base.py:354\u001b[39m, in \u001b[36mImageProcessingMixin.get_image_processor_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    333\u001b[39m     resolved_image_processor_files = [\n\u001b[32m    334\u001b[39m         resolved_file\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m [image_processor_file, PROCESSOR_NAME]\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    353\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     resolved_image_processor_file = \u001b[43mresolved_image_processor_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m    356\u001b[39m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl13/lib/python3.13/site-packages/transformers/models/auto/image_processing_auto.py:485\u001b[39m, in \u001b[36mAutoImageProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     config_dict, _ = \u001b[43mImageProcessingMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_processor_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl13/lib/python3.13/site-packages/transformers/image_processing_base.py:361\u001b[39m, in \u001b[36mImageProcessingMixin.get_image_processor_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    360\u001b[39m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    362\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt load image processor for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. If you were trying to load\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    363\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m it from \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, make sure you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have a local directory with the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    364\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m same name. Otherwise, make sure \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is the correct path to a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    365\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m directory containing a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_processor_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m file\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    366\u001b[39m         )\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    369\u001b[39m     \u001b[38;5;66;03m# Load image_processor dict\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: Can't load image processor for 'mahmoodLab/conchv1_5'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mahmoodLab/conchv1_5' is the correct path to a directory containing a config.json file",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoImageProcessor\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 1. Load the processor (handles normalization/resizing)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m processor = \u001b[43mAutoImageProcessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmahmoodLab/conchv1_5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 2. Load the Model \u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Using trust_remote_code=True is required for custom architectures like CONCH\u001b[39;00m\n\u001b[32m     12\u001b[39m model = AutoModel.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mMahmoodLab/conchv1_5\u001b[39m\u001b[33m\"\u001b[39m, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl13/lib/python3.13/site-packages/transformers/models/auto/image_processing_auto.py:489\u001b[39m, in \u001b[36mAutoImageProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    485\u001b[39m     config_dict, _ = ImageProcessingMixin.get_image_processor_dict(\n\u001b[32m    486\u001b[39m         pretrained_model_name_or_path, image_processor_filename=CONFIG_NAME, **kwargs\n\u001b[32m    487\u001b[39m     )\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m initial_exception\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# In case we have a config_dict, but it's not a timm config dict, we raise the initial exception,\u001b[39;00m\n\u001b[32m    492\u001b[39m \u001b[38;5;66;03m# because only timm models have image processing in `config.json`.\u001b[39;00m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_timm_config_dict(config_dict):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl13/lib/python3.13/site-packages/transformers/models/auto/image_processing_auto.py:476\u001b[39m, in \u001b[36mAutoImageProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# Load the image processor config\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# Main path for all transformers models and local TimmWrapper checkpoints\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     config_dict, _ = \u001b[43mImageProcessingMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_processor_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_processor_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m initial_exception:\n\u001b[32m    480\u001b[39m     \u001b[38;5;66;03m# Fallback path for Hub TimmWrapper checkpoints. Timm models' image processing is saved in `config.json`\u001b[39;00m\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# instead of `preprocessor_config.json`. Because this is an Auto class and we don't have any information\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# except the model name, the only way to check if a remote checkpoint is a timm model is to try to\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;66;03m# load `config.json` and if it fails with some error, we raise the initial exception.\u001b[39;00m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl13/lib/python3.13/site-packages/transformers/image_processing_base.py:361\u001b[39m, in \u001b[36mImageProcessingMixin.get_image_processor_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    360\u001b[39m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    362\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt load image processor for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. If you were trying to load\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    363\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m it from \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, make sure you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have a local directory with the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    364\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m same name. Otherwise, make sure \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is the correct path to a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    365\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m directory containing a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_processor_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m file\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    366\u001b[39m         )\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    369\u001b[39m     \u001b[38;5;66;03m# Load image_processor dict\u001b[39;00m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(resolved_image_processor_file, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n",
      "\u001b[31mOSError\u001b[39m: Can't load image processor for 'mahmoodLab/conchv1_5'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mahmoodLab/conchv1_5' is the correct path to a directory containing a preprocessor_config.json file"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import torch\n",
    "import timm \n",
    "from torchvision import transforms\n",
    "from transformers import AutoModel, AutoImageProcessor\n",
    "\n",
    "# 1. Load the processor (handles normalization/resizing)\n",
    "processor = AutoImageProcessor.from_pretrained(\"ahmoodLab/conchv1_5\", trust_remote_code=True)\n",
    "\n",
    "# 2. Load the Model \n",
    "# Using trust_remote_code=True is required for custom architectures like CONCH\n",
    "model = AutoModel.from_pretrained(\"MahmoodLab/conchv1_5\", trust_remote_code=True)\n",
    "\n",
    "# Login to the Hugging Face hub, using your user access token that can be found here:\n",
    "# https://huggingface.co/settings/tokens.\n",
    "login(\"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\")\n",
    "\n",
    "# model = timm.create_model(\n",
    "#     \"hf-hub:bioptimus/H-optimus-0\", pretrained=True, init_values=1e-5, dynamic_img_size=False\n",
    "# )\n",
    "# model = timm.create_model(\n",
    "#     \"hf-hub:MahmoodLab/conchv1_5\", pretrained=True, init_values=1e-5, dynamic_img_size=False\n",
    "# )\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "trnsfrms_val = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.707223, 0.578729, 0.703617), \n",
    "        std=(0.211883, 0.230117, 0.177517)\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ecd8a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1536])\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "# from CHIEF.models.ctran import ctranspath\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from PIL import Image, ImageFile, PngImagePlugin\n",
    "Image.MAX_IMAGE_PIXELS = None \n",
    "PngImagePlugin.MAX_TEXT_CHUNK = 100 * 1024 * 1024  # 100MB\n",
    "PngImagePlugin.MAX_TEXT_MEMORY = 100 * 1024 * 1024 # 100MB\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "image = Image.open(\"./test_img.png\").convert(\"RGB\")\n",
    "image = trnsfrms_val(image).unsqueeze(dim=0)\n",
    "with torch.no_grad():\n",
    "    patch_feature_emb = model(image) # Extracted features (torch.Tensor) with shape [1,768]\n",
    "    print(patch_feature_emb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6549b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as j_\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(j_(dataDir, 'train'), transform=trnsfrms_val)\n",
    "test_dataset = torchvision.datasets.ImageFolder(j_(dataDir, 'test'), transform=trnsfrms_val)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=False, num_workers=16)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5816c3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1719ec3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6e99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd0bee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
