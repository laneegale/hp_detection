{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa00761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n trident python=3.10\n",
    "# cd trident\n",
    "# pip install -e .\n",
    "# pip install ipynbname\n",
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2117c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynbname\n",
    "import os\n",
    "\n",
    "this_notebook_name = ipynbname.name()\n",
    "feats_save_dir = \"feats_h5\"\n",
    "\n",
    "if not os.path.exists(feats_save_dir):\n",
    "    os.mkdir(feats_save_dir)\n",
    "\n",
    "import json\n",
    "import h5py\n",
    "import logging\n",
    "\n",
    "from PIL import Image, ImageFile, PngImagePlugin\n",
    "Image.MAX_IMAGE_PIXELS = None \n",
    "PngImagePlugin.MAX_TEXT_CHUNK = 100 * 1024 * 1024  # 100MB\n",
    "PngImagePlugin.MAX_TEXT_MEMORY = 100 * 1024 * 1024 # 100MB\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torch.multiprocessing\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import timm\n",
    "from trident.patch_encoder_models import encoder_factory\n",
    "\n",
    "dataDir = Path(\"/media/toom/New Volume/cuhk_data/HPACG_dataHPACG_split\")\n",
    "train_positive_dir = dataDir / \"train/positive\"\n",
    "train_negative_dir = dataDir / \"train/negative\"\n",
    "\n",
    "test_positive_dir = dataDir / \"test/positive\"\n",
    "test_negative_dir = dataDir / \"test/negative\"\n",
    "\n",
    "def get_random_img_path():\n",
    "    rand_img = random.choice(os.listdir(dataDir / \"train/positive\"))\n",
    "    rand_img_full_path = dataDir / \"train/positive\" / rand_img\n",
    "\n",
    "    return rand_img_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d1859cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = encoder_factory(model_name='conch_v15')\n",
    "\n",
    "    def get_eval_transforms_conchv1_5(img_resize: int = 448):\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(\n",
    "                    img_resize, interpolation=transforms.InterpolationMode.BICUBIC\n",
    "                ),\n",
    "                transforms.CenterCrop(img_resize),\n",
    "                transforms.Lambda(\n",
    "                    lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        )\n",
    "        return transform\n",
    "\n",
    "    transform = get_eval_transforms_conchv1_5()\n",
    "\n",
    "    return model, transform\n",
    "\n",
    "def run_model_and_generate_attention_mask(img_path):\n",
    "    model, transform = get_model()\n",
    "    model.to('cpu')\n",
    "    _ = model.eval()\n",
    "\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    o_img = img.copy()\n",
    "    img = transform(img).unsqueeze(dim=0).to('cpu')\n",
    "\n",
    "    attention_scores = []\n",
    "    hooks = []\n",
    "\n",
    "    def get_attention_matrix(module, input, output):\n",
    "        attention_scores.append(output.detach().cpu())\n",
    "\n",
    "    for i in range(24):\n",
    "        target_layer = model.model.trunk.blocks[i].attn\n",
    "        hook = target_layer.register_forward_hook(get_attention_matrix)\n",
    "        hooks.append(hook)\n",
    "\n",
    "    # block = model.model.trunk.blocks[-1]\n",
    "    # def block_hook(module, input, output):\n",
    "    #     print(\"Block executed!\")\n",
    "    #     print(f\"Input shape: {input[0].shape}\")\n",
    "    # handle = block.register_forward_hook(block_hook)\n",
    "\n",
    "    _ = model(img)\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    o_img = np.array(o_img)\n",
    "    attentions = attention_scores[-1][:, 1:, :]\n",
    "    heatmap = torch.norm(attentions, dim=-1)\n",
    "    heatmap = heatmap.reshape(28, 28).detach().numpy()\n",
    "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "    heatmap = heatmap ** 4\n",
    "    # heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "\n",
    "    smoothed_heatmap = scipy.ndimage.gaussian_filter(heatmap, sigma=1.5)\n",
    "    smoothed_heatmap = cv2.resize(smoothed_heatmap, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "    # smoothed_heatmap[smoothed_heatmap < smoothed_heatmap.mean()] *= 0.7\n",
    "    H, W = smoothed_heatmap.shape\n",
    "\n",
    "    overlay = np.zeros((H, W, 4))\n",
    "\n",
    "    overlay[..., 0] = 1.0  # Red\n",
    "    overlay[..., 1] = 0.0  # Green\n",
    "    overlay[..., 2] = 0.0  # Blue\n",
    "    overlay[..., 3] = smoothed_heatmap \n",
    "\n",
    "    def plot_side_by_side(img, overlay):\n",
    "        # Create a figure with 1 row and 2 columns\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 6))\n",
    "\n",
    "        # Plot first image\n",
    "        axes[0].imshow(img)\n",
    "        axes[0].set_title(\"Input\")\n",
    "        axes[0].axis('off') # Hide tick numbers\n",
    "\n",
    "        # Plot second image\n",
    "        axes[1].imshow(img)\n",
    "        axes[1].imshow(overlay)\n",
    "        axes[1].set_title(\"Overlay\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        axes[2].imshow(overlay)\n",
    "        axes[2].set_title(\"Attention Mask\")\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        plt.tight_layout() # Adjusts spacing so they don't overlap\n",
    "        plt.show()\n",
    "\n",
    "    # Usage\n",
    "    plot_side_by_side(o_img, overlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b32be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "class CustomDataset(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths.\"\"\"\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        original_tuple = super().__getitem__(index)\n",
    "        image = original_tuple[0]\n",
    "        label = original_tuple[1]\n",
    "        \n",
    "        # Get the file path of the image\n",
    "        img_path = self.samples[index][0]\n",
    "        \n",
    "        return (image, label, os.path.basename(img_path))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def custom_extract_patch_features_from_dataloader(model, dataloader, save_dir=None):\n",
    "    \"\"\" Modified from uni.downstream.extract_patch_features.extract_patch_features_from_dataloader\n",
    "        Uses model to extract features+labels from images iterated over the dataloader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn): torch.nn CNN/VIT architecture with pretrained weights that extracts d-dim features.\n",
    "        dataloader (torch.utils.data.DataLoader): torch.utils.data.DataLoader object of N images.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary object that contains (1) [N x D]-dim np.array of feature embeddings, and (2) [N x 1]-dim np.array of labels\n",
    "\n",
    "    \"\"\"\n",
    "    torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "\n",
    "    # all_embeddings, all_labels, all_filenames = [], [], []\n",
    "    batch_size = dataloader.batch_size\n",
    "    try:\n",
    "        device = next(model.parameters())[0].device\n",
    "    except:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    if save_dir is None:\n",
    "        h5_file_path = os.path.join(feats_save_dir, this_notebook_name+'.h5')\n",
    "    else:\n",
    "        h5_file_path = save_dir\n",
    "\n",
    "    with h5py.File(h5_file_path, 'a') as hf:\n",
    "        for batch_idx, (batch, target, filenames) in tqdm(\n",
    "            enumerate(dataloader), total=len(dataloader)\n",
    "        ):\n",
    "            if filenames[0] in hf and filenames[-1] in hf:\n",
    "                continue\n",
    "\n",
    "            remaining = batch.shape[0]\n",
    "            if remaining != batch_size:\n",
    "                _ = torch.zeros((batch_size - remaining,) + batch.shape[1:]).type(\n",
    "                    batch.type()\n",
    "                )\n",
    "                batch = torch.vstack([batch, _])\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            with torch.inference_mode():\n",
    "                embeddings = model(batch).detach().cpu()[:remaining, :]\n",
    "                labels = target.numpy()[:remaining]\n",
    "                assert not torch.isnan(embeddings).any()\n",
    "\n",
    "            for i in range(len(filenames)):\n",
    "                if filenames[i] in hf:\n",
    "                    continue\n",
    "                dset = hf.create_dataset(filenames[i], data=embeddings[i].numpy())\n",
    "                dset.attrs[\"label\"] = labels[i]\n",
    "\n",
    "            # all_embeddings.append(embeddings)\n",
    "            # all_labels.append(labels)\n",
    "            # all_filenames.append(filename)\n",
    "\n",
    "    return None\n",
    "    # asset_dict = {\n",
    "    #     \"embeddings\": np.vstack(all_embeddings).astype(np.float32),\n",
    "    #     \"labels\": np.concatenate(all_labels),\n",
    "    # }\n",
    "\n",
    "    # return asset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699c94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toom/miniconda3/envs/trident/lib/python3.10/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from os.path import join as j_\n",
    "from UNI.uni.downstream.extract_patch_features import extract_patch_features_from_dataloader\n",
    "from UNI.uni.downstream.eval_patch_features.linear_probe import eval_linear_probe\n",
    "\n",
    "model, trnsfrms_val = get_model()\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "train_dataset = CustomDataset(dataDir / 'train', transform=trnsfrms_val)\n",
    "test_dataset = CustomDataset(dataDir / 'test', transform=trnsfrms_val)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=False, num_workers=16)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65350a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_extract_patch_features_from_dataloader(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = extract_patch_features_from_dataloader(model, train_dataloader)\n",
    "test_features = extract_patch_features_from_dataloader(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80182de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats = torch.Tensor(train_features['embeddings'])\n",
    "train_labels = torch.Tensor(train_features['labels']).type(torch.long)\n",
    "test_feats = torch.Tensor(test_features['embeddings'])\n",
    "test_labels = torch.Tensor(test_features['labels']).type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30086fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from UNI.uni.downstream.eval_patch_features.metrics import get_eval_metrics, print_metrics\n",
    "\n",
    "linprobe_eval_metrics, linprobe_dump = eval_linear_probe(\n",
    "    train_feats = train_feats,\n",
    "    train_labels = train_labels,\n",
    "    valid_feats = None ,\n",
    "    valid_labels = None,\n",
    "    test_feats = test_feats,\n",
    "    test_labels = test_labels,\n",
    "    max_iter = 1000,\n",
    "    verbose= True,\n",
    ")\n",
    "\n",
    "print_metrics(linprobe_eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3929d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "y_true = linprobe_dump['preds_all']\n",
    "y_pred = linprobe_dump['targets_all']\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "labels = ['non-hpacg', 'hpacg']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix=cm.ravel()\n",
    "\n",
    "tpr = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "specificity = TN / (TN + FP) if (FP + TN) > 0 else 0\n",
    "Balanced_Accuracy = (specificity + tpr) / 2\n",
    "Precision = TP  / (TP + FP) if (TP + FP) > 0 else 0\n",
    "Weight_F1 = ((Precision * tpr) / (Precision + tpr))*2\n",
    "\n",
    "print(f\"True Positive Rate (Sensitivity,recall): [{tpr:.3f}]\")\n",
    "print(f\"False Positive Rate : [{fpr:.3f}]\")\n",
    "print(f\"Specificity : [{specificity:.3f}]\")\n",
    "print(f\"Balanced Accuracy : [{Balanced_Accuracy:.3f}]\")\n",
    "print(f\"Precision: [{Precision:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67fc8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "true_labels = linprobe_dump['targets_all']\n",
    "pred_probs = linprobe_dump['probs_all']\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(true_labels, pred_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'UNI (AUC={roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')  \n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c438bf4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b64a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trident",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
